{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dcicutils import ff_utils\n",
    "from functions.wfr import *\n",
    "from functions.wfr_settings import *\n",
    "from functions.notebook_functions import *\n",
    "# tibanna = Tibanna(env=env)\n",
    "my_env = 'data'\n",
    "my_auth = get_key('koray_data')\n",
    "\n",
    "# different types of exps use different steps at the last step(3).\n",
    "recipe = [\n",
    "    [['in%20situ%20Hi-C', 'dilution%20Hi-C'], 'hi-c-processing-pairs'],\n",
    "    [['micro-C',          'DNase%20Hi-C'],    'hi-c-processing-pairs-nore'],\n",
    "    [['capture%20Hi-C',   'PLAC-seq'],        'hi-c-processing-pairs-nonorm'],\n",
    "    [['CHIA-pet',         'TrAC-loop'],       'hi-c-processing-pairs-nore-nonorm'],\n",
    "    [['TCC'], 'hi-c-processing-pairs'],\n",
    "    \n",
    "]\n",
    "\n",
    "# To Do assign core 8 and more memory (\"instance_type\": \"c4.4xlarge\",) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/search/?experiments_in_set.experiment_type=micro-C&experiments_in_set.experiment_type=DNase%20Hi-C&type=ExperimentSetReplicate&limit=all&status=pre-release&status=released&status=released%20to%20project\n",
      "3 total number of sets\n",
      "0 sets completed\n",
      "0 sets skipped for small size\n",
      "3 ready for processing\n",
      "\n",
      "1 4DNESNGESP28 None human 77\n",
      "4DNEXRJN8UYK has missing Part1 runs\n",
      "4DNEXLLYE3XZ has missing Part1 runs\n",
      "4DNEX21WVKWL part1 complete\n",
      "4DNEX21WVKWL is missing Part2\n",
      "4DNEXWXUC1X2 part1 complete\n",
      "4DNEXWXUC1X2 is missing Part2\n",
      "Part3 not ready\n",
      "\n",
      "2 4DNESTCJSP7W None human 110\n",
      "4DNEXJG75QSO has missing Part1 runs\n",
      "4DNEXUUUTV7X has missing Part1 runs\n",
      "4DNEXKCVV4IB has missing Part1 runs\n",
      "Part3 not ready\n",
      "\n",
      "3 4DNESJC437VS None human 5\n",
      "4DNEX21WVKWL part1 complete\n",
      "4DNEX21WVKWL is missing Part2\n",
      "4DNEXWXUC1X2 part1 complete\n",
      "4DNEXWXUC1X2 is missing Part2\n",
      "Part3 not ready\n",
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "## TODO make sure set_url is compatible with the set_url sets\n",
    "\n",
    "#Choose the recipe element to run the pipeline on\n",
    "recipe_no = 0\n",
    "exp_type, step3 = recipe[recipe_no]\n",
    "\n",
    "#Choose the type of operations you want\n",
    "add_pc = False    #add processed files to 'other processed files\n",
    "add_tag = False   #add the completed process tag if done with all steps\n",
    "add_wfr = False   #start missing wfrs\n",
    "add_tag_small = True   # add skipped small tag\n",
    "\n",
    "set_url = '/search/?'+ \\\n",
    "          '&'.join(['experiments_in_set.experiment_type='+i for i in exp_type])+ \\\n",
    "          '&type=ExperimentSetReplicate&limit=all' + \\\n",
    "          '&status=pre-release&status=released&status=released%20to%20project'\n",
    "\n",
    "print(set_url)\n",
    "\n",
    "\n",
    "set_url = '/search/?award.project=External&experiments_in_set.experiment_type=in+situ+Hi-C&experimentset_type=replicate&lab.display_title=Rafael+Casellas%2C+NIH&type=ExperimentSetReplicate'\n",
    "print(set_url)\n",
    "#set_url = '/search/?award.project=ENCODE&experimentset_type=replicate&type=ExperimentSetReplicate'\n",
    "\n",
    "#set_url = '/search/?award.project=4DN&experiments_in_set.biosample.biosource.individual.organism.name=fruit-fly&experimentset_type=replicate&type=ExperimentSetReplicate'\n",
    "#print set_url\n",
    "\n",
    "all_sets = ff_utils.search_metadata(set_url , key=my_auth)\n",
    "counter = 0\n",
    "completed = 0\n",
    "completed_acc = []\n",
    "\n",
    "uncompleted_sets = [i for i in all_sets if \"HiC_Pipeline_0.2.5\"  not in i.get('completed_processes', [])]\n",
    "run_sets = [i for i in uncompleted_sets if \"HiC_Pipeline_0.2.5-skipped-small-set\"  not in i.get('completed_processes', [])]\n",
    "\n",
    "print(len(all_sets), 'total number of sets')\n",
    "print(len(all_sets)-len(uncompleted_sets), 'sets completed')\n",
    "print(len(uncompleted_sets)-len(run_sets), 'sets skipped for small size')\n",
    "print(len(run_sets), 'ready for processing')\n",
    "for a_set in run_sets: \n",
    "    \n",
    "    attributions = None\n",
    "    print()\n",
    "    counter += 1\n",
    "\n",
    "        \n",
    "    fastqpairs, organism, enzyme, bwa_ref, chrsize_ref, enz_ref, f_size, lab = find_pairs(a_set, my_env)\n",
    "    # skip based on these conditions\n",
    "    if not bwa_ref or not chrsize_ref:\n",
    "        print(counter, a_set['accession'], organism, enzyme, 'skipping set with no chrsize/bwa index')\n",
    "        continue\n",
    "    if 'nonorm' not in step3:\n",
    "        if f_size < 4:\n",
    "            print(counter, a_set['accession'], 'skipping small file size', str(f_size))\n",
    "            if add_tag_small:\n",
    "                ff_utils.patch_metadata({\"completed_processes\":[\"HiC_Pipeline_0.2.5-skipped-small-set\"]}, obj_id=a_set['accession'] , key=my_auth)\n",
    "                \n",
    "            continue\n",
    "    if 'nore' not in step3:\n",
    "        if not enz_ref:\n",
    "            print(counter, a_set['accession'], 'skipping not ready NZ', organism, enzyme)\n",
    "            continue\n",
    "    print(counter, a_set['accession'],enzyme, organism,f_size)\n",
    "    part3 = 'done'\n",
    "    set_pairs = []        \n",
    "    # cycle through the experiments, skip the ones without usable files\n",
    "    for exp in fastqpairs.keys():\n",
    "        if not fastqpairs.get(exp):\n",
    "            print(exp, 'does not have any fastq pairs')\n",
    "            continue\n",
    "        # Check Part 1 and See if all are okay\n",
    "        exp_bams = []\n",
    "        part1 = 'done'\n",
    "        part2 = 'done'\n",
    "        \n",
    "        for pair in fastqpairs[exp]:\n",
    "            #############\n",
    "            if not attributions:\n",
    "                attributions = get_attribution(ff_utils.get_metadata(pair[0], key = my_auth))\n",
    "            step1_result = get_wfr_out(pair[0], 'bwa-mem 0.2.5', my_auth)\n",
    " \n",
    "            # if successful\n",
    "            if step1_result['status'] == 'complete':\n",
    "                exp_bams.append(step1_result['bam'])\n",
    "                continue\n",
    "            # if still running\n",
    "            elif step1_result['status'] == 'running':\n",
    "                part1 = 'not done'\n",
    "                print('part1 still running')\n",
    "                continue\n",
    "            # if run is not successful\n",
    "            else:\n",
    "                part1 = 'not done'\n",
    "                if add_wfr:\n",
    "                    # RUN PART 1\n",
    "                    inp_f = {'fastq1':pair[0], 'fastq2':pair[1], 'bwa_index':bwa_ref}\n",
    "                    name_tag = pair[0].split('/')[2]+'_'+pair[1].split('/')[2]\n",
    "                    run_missing_wfr(step_settings('bwa-mem', organism, attributions), inp_f, name_tag, my_auth, my_env)\n",
    "        # stop progress to part2 \n",
    "        if part1 is not 'done':\n",
    "            print(exp, 'has missing Part1 runs')\n",
    "            part2 = 'not ready'\n",
    "            part3 = 'not ready'\n",
    "            continue\n",
    "        print(exp, 'part1 complete')\n",
    "           \n",
    "        #make sure all input bams went through same last step2\n",
    "        all_step2s = []\n",
    "        for bam in exp_bams:\n",
    "            step2_result = get_wfr_out(bam, 'hi-c-processing-bam 0.2.5', my_auth)\n",
    "            all_step2s.append((step2_result['status'],step2_result.get('bam')))\n",
    "        if len(list(set(all_step2s))) != 1:\n",
    "            print('inconsistent step2 run for input bams')\n",
    "            # this run will be repeated if add_wfr\n",
    "            step2_result['status'] = 'inconsistent run'\n",
    "            \n",
    "        #check if part 2 is run already, it not start the run\n",
    "        # if successful\n",
    "        if step2_result['status'] == 'complete':\n",
    "            set_pairs.append(step2_result['pairs'])\n",
    "            if add_pc:\n",
    "                add_preliminary_processed_files(exp, [step2_result['bam'],step2_result['pairs']], my_auth)\n",
    "            print(exp, 'part2 complete')\n",
    "            continue\n",
    "        # if still running\n",
    "        elif step2_result['status'] == 'running':\n",
    "            part2 = 'not done'\n",
    "            part3 = 'not ready'\n",
    "            print(exp, 'part2 still running')\n",
    "            continue\n",
    "        # if run is not successful\n",
    "        else:\n",
    "            part2 = 'not done'\n",
    "            part3 = 'not ready'\n",
    "            print(exp, 'is missing Part2')\n",
    "            if add_wfr:\n",
    "                # RUN PART 2\n",
    "                inp_f = {'input_bams':exp_bams, 'chromsize':chrsize_ref}           \n",
    "                run_missing_wfr(step_settings('hi-c-processing-bam', organism, attributions), inp_f, exp, my_auth, my_env) \n",
    "\n",
    "                \n",
    "    if part3 is not 'done':\n",
    "        print('Part3 not ready')\n",
    "        continue\n",
    "    if not set_pairs:\n",
    "        print('no pairs can be produced from this set')\n",
    "        continue\n",
    "\n",
    "    #make sure all input bams went through same last step3\n",
    "    all_step3s = []\n",
    "    for a_pair in set_pairs:\n",
    "        step3_result = get_wfr_out(a_pair, step3 + \" 0.2.5\", my_auth)\n",
    "        all_step3s.append((step3_result['status'], step3_result.get('mcool')))\n",
    "    if len(list(set(all_step3s))) != 1:\n",
    "        print('inconsistent step3 run for input pairs')\n",
    "        # this run will be repeated if add_wfr\n",
    "        step3_result['status'] = 'inconsistent run'\n",
    "    #check if part 3 is run already, it not start the run\n",
    "    # if successful\n",
    "    if step3_result['status'] == 'complete':\n",
    "        completed += 1\n",
    "        completed_acc.append(a_set['accession'])\n",
    "        #add competed flag to experiment\n",
    "        if add_tag:\n",
    "            ff_utils.patch_metadata({\"completed_processes\":[\"HiC_Pipeline_0.2.5\"]}, obj_id=a_set['accession'] , key=my_auth)\n",
    "        # add processed files to set\n",
    "        if add_pc:\n",
    "            add_preliminary_processed_files(a_set['accession'], \n",
    "                                            [step3_result['pairs'],\n",
    "                                             step3_result['hic'],\n",
    "                                             step3_result['mcool']], \n",
    "                                            my_auth)\n",
    "        print(a_set['accession'], 'part3 complete')\n",
    "    # if still running\n",
    "    elif step3_result['status'] == 'running':\n",
    "        print('part3 still running')\n",
    "        continue\n",
    "    # if run is not successful\n",
    "    else:\n",
    "        print(a_set['accession'], 'is missing Part3')\n",
    "        if add_wfr:\n",
    "            # RUN PART 3\n",
    "            inp_f = {'input_pairs':set_pairs, 'chromsizes':chrsize_ref}\n",
    "            if recipe_no in [0,2,4]:\n",
    "                inp_f['restriction_file'] = enz_ref\n",
    "            run_missing_wfr(step_settings(step3, organism, attributions), inp_f, a_set['accession'], my_auth, my_env)\n",
    "\n",
    "print(completed)\n",
    "print(completed_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3 4DNESH4UTRNL DpnII mouse 800  -  4DNEX4KRGMAQ is missing Part2 - Fails at runtaskawsem\n",
    "https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:hi-c-processing-bam_4DNEX4KRGMAQ9b484528-5651-4d72-a271-a9b37a42ab05\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/search/?experiments_in_set.experiment_type=in%20situ%20Hi-C&experiments_in_set.experiment_type=dilution%20Hi-C&type=ExperimentSetReplicate&limit=all&status=released&status=released%20to%20project\n",
      "4\n",
      "4DNES76KXUJ3\n",
      "4DNESNLXBFMY\n",
      "4DNESIU6F8HF\n",
      "4DNESSQU7B76\n",
      "4 items are ready\n"
     ]
    }
   ],
   "source": [
    "# Move files from opc to pc\n",
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "from functions.wfr import *\n",
    "\n",
    "recipe_no = 0\n",
    "exp_type, step3 = recipe[recipe_no]\n",
    "action = False\n",
    "move_title = 'HiC Processing Pipeline - Preliminary Files'\n",
    "\n",
    "# set_url = '/search/?'+ \\\n",
    "#           '&'.join(['experiments_in_set.experiment_type='+i for i in exp_type])+ \\\n",
    "#           '&type=ExperimentSetReplicate&limit=all' + \\\n",
    "#           '&status=released&status=released%20to%20project'\n",
    "\n",
    "# print(set_url)\n",
    "# set_url = '/search/?award.project=External&experimentset_type=replicate&lab.display_title=Benjamin+Rowland%2C+NKI&type=ExperimentSetReplicate'\n",
    "\n",
    "# exp\n",
    "# set_url = '/search/?'+ \\\n",
    "#           '&'.join(['experiment_type='+i for i in exp_type])+ \\\n",
    "#           '&type=Experiment&limit=all' + \\\n",
    "#           '&status=released&status=released%20to%20project'\n",
    "\n",
    "all_sets = ff_utils.search_metadata(set_url , key=my_auth)\n",
    "\n",
    "ready_sets_1 = [i for i in all_sets if \"HiC_Pipeline_0.2.5\" in i.get('completed_processes', [])]\n",
    "print(len(ready_sets_1))\n",
    "ready_sets_2 = []\n",
    "for a_set in ready_sets_1:\n",
    "    if a_set.get('other_processed_files'):\n",
    "        print(a_set['accession'])\n",
    "        if move_title in [i['title'] for i in a_set['other_processed_files']]:\n",
    "            if a_set.get('processed_files'):\n",
    "                print('WARN' ,a_set['accession'], 'has items in processed files, skipping ')\n",
    "                continue\n",
    "            else:\n",
    "                ready_sets_2.append(a_set)\n",
    "print(len(ready_sets_2), 'items are ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 experiment sets in scope\n",
      "1 4DNES76KXUJ3\n",
      "4DNES76KXUJ3 files will move\n",
      "4DNES76KXUJ3 moved to pc\n",
      "4DNEXFB9M8MX files will move\n",
      "4DNEXFB9M8MX moved to pc\n",
      "4DNEXRN5PGA6 files will move\n",
      "4DNEXRN5PGA6 moved to pc\n",
      "4DNEXJAQSA8D files will move\n",
      "4DNEXJAQSA8D moved to pc\n",
      "4DNEX7LV55MZ files will move\n",
      "4DNEX7LV55MZ moved to pc\n",
      "4DNEXFPWLYHY files will move\n",
      "4DNEXFPWLYHY moved to pc\n",
      "\n",
      "2 4DNESNLXBFMY\n",
      "4DNESNLXBFMY files will move\n",
      "4DNESNLXBFMY moved to pc\n",
      "4DNEXV5HTNBS files will move\n",
      "4DNEXV5HTNBS moved to pc\n",
      "4DNEXUYGDZDK files will move\n",
      "4DNEXUYGDZDK moved to pc\n",
      "4DNEXUL8YRCT files will move\n",
      "4DNEXUL8YRCT moved to pc\n",
      "\n",
      "3 4DNESIU6F8HF\n",
      "4DNESIU6F8HF files will move\n",
      "4DNESIU6F8HF moved to pc\n",
      "4DNEXMF4CO8O files will move\n",
      "4DNEXMF4CO8O moved to pc\n",
      "4DNEXZPB8C7E files will move\n",
      "4DNEXZPB8C7E moved to pc\n",
      "4DNEX25INNTK files will move\n",
      "4DNEX25INNTK moved to pc\n",
      "\n",
      "4 4DNESSQU7B76\n",
      "4DNESSQU7B76 files will move\n",
      "4DNESSQU7B76 moved to pc\n",
      "4DNEXKKWDEQW files will move\n",
      "4DNEXKKWDEQW moved to pc\n",
      "4DNEXNGO8B29 files will move\n",
      "4DNEXNGO8B29 moved to pc\n",
      "4DNEXS21OUF6 files will move\n",
      "4DNEXS21OUF6 moved to pc\n",
      "\n",
      "4\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# move other processed files to processed files field\n",
    "action = True\n",
    "def move_opc_to_pc(resp, move_title, con_key):\n",
    "    opc = resp.get('other_processed_files')\n",
    "    pc = resp.get('processed_files')\n",
    "    # if processed_files field already has values, exit\n",
    "    if pc:\n",
    "        if opc:\n",
    "            print('There are files in processed_files field, expected empty', resp['accession'])\n",
    "            return False\n",
    "        else:\n",
    "            print('it is possible that move already happened, no opc but pc', resp['accession'])\n",
    "    # see if there are other_processed_files to move\n",
    "    if opc:\n",
    "        titles = [i['title'] for i in opc]\n",
    "        if move_title in titles:\n",
    "            print(resp['accession'], 'files will move')\n",
    "            move_item = [i for i in opc if i['title'] == move_title]\n",
    "            assert len(move_item) == 1\n",
    "            assert move_item[0]['type'] == 'preliminary'\n",
    "            new_pc = move_item[0]['files']\n",
    "            new_opc = [i for i in opc if i['title'] != move_title]\n",
    "            # Time to patch\n",
    "            patch_data = {}\n",
    "            add_on = \"\"\n",
    "            #if there is something left in opc, patch it, if not delete field\n",
    "            if new_opc:\n",
    "                patch_data['other_processed_files'] = opc\n",
    "            else:\n",
    "                add_on = 'delete_fields=other_processed_files'\n",
    "            # patch with processed files\n",
    "            patch_data['processed_files'] = new_pc\n",
    "            if action:\n",
    "                ff_utils.patch_metadata(patch_data, resp['uuid'], key = con_key, add_on = add_on)\n",
    "                # update status of pc to status of set or exp\n",
    "                release_files(resp['uuid'], new_pc, con_key)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "        \n",
    "set_w_apf = 0\n",
    "exp_w_apf = 0\n",
    "counter = 0\n",
    "move_title = 'HiC Processing Pipeline - Preliminary Files'\n",
    "\n",
    "print(len(ready_sets_2), 'experiment sets in scope')\n",
    "for a_set in ready_sets_2:\n",
    "    set_resp = ff_utils.get_metadata(a_set['uuid'],key=my_auth, add_on='frame=raw')\n",
    "    counter += 1\n",
    "    print(counter, set_resp['accession'])\n",
    "    exps = set_resp['experiments_in_set']\n",
    "    res =  move_opc_to_pc(set_resp, move_title, my_auth)\n",
    "    if res:\n",
    "        set_w_apf += 1\n",
    "        print(set_resp['accession'], 'moved to pc')\n",
    "  \n",
    "    for exp in exps:\n",
    "        exp_resp = ff_utils.get_metadata(exp, key=my_auth, add_on='frame=raw')\n",
    "        res_e =  move_opc_to_pc(exp_resp,move_title,my_auth)\n",
    "        if res_e:\n",
    "            exp_w_apf += 1\n",
    "            print(exp_resp['accession'], 'moved to pc')\n",
    "    print()\n",
    "\n",
    "print(set_w_apf)\n",
    "print(exp_w_apf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
