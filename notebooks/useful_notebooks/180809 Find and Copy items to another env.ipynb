{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLEASE COPY NOTEBOOKS TO YOUR FOLDERS TO PREVENT COMMIT CONFLICTS\n",
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "import json\n",
    "\n",
    "# get key from keypairs.json\n",
    "my_env = 'data'\n",
    "my_key = get_key('koray_data')\n",
    "schema_name = get_schema_names(my_key) \n",
    "print('WORKING ON', my_key['server'], '\\n')\n",
    "\n",
    "##### COLLECT ITEMS TO Release #####\n",
    "# use either a starting item to fetch all linked items\n",
    "\n",
    "# Use a starting item to find linked ones\n",
    "# starting_items = ['46db06ad-b399-4cf4-9acc-07b3e25ef132']\n",
    "#add_items = get_query_or_linked(my_key, linked=starting_items)\n",
    "\n",
    "# or a search query\n",
    "#my_query = '/search/?q=GOLD&type=Item&limit=all'\n",
    "#add_items = get_query_or_linked(my_key, query=my_query)\n",
    "\n",
    "# if you want you can dump them to separate json files (will work as test insert)\n",
    "# dump_to_json(add_items, destination folder)\n",
    "\n",
    "my_query = '/search/?biosample.biosource.individual.organism.name=mouse&biosample.biosource_summary=ES-E14&experiment_type=in%20situ%20Hi-C&type=ExperimentHiC'\n",
    "store = get_query_or_linked(my_key, query=my_query, linked_frame='raw')\n",
    "print(store.keys())\n",
    "print(len([i['uuid'] for key in store for i in store[key]]))\n",
    "print()\n",
    "\n",
    "find_linked = ['48732435-5a16-4d86-a0f6-ace18dc62b6c']\n",
    "store = get_query_or_linked(my_key, linked=find_linked, linked_frame='raw')\n",
    "print(store.keys())\n",
    "print(len([i['uuid'] for key in store for i in store[key]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This part should only run once!\n",
    "\n",
    "transfer_env = 'fourfront-mastertest'\n",
    "transfer_key = ff_utils.get_authentication_with_server({}, ff_env=transfer_env)\n",
    "# reverse lookup dictionary for schema names\n",
    "\n",
    "# if the item exist in the target, should it overwrite it (will include user/award etc)\n",
    "overwrite_existing = False\n",
    "\n",
    "rev_schema_name = {}\n",
    "for key, name in schema_name.items():\n",
    "    rev_schema_name[name] = schema_name[key]\n",
    "\n",
    "my_types = [i for i in ORDER if i in store.keys()]\n",
    "\n",
    "second_round_items = {}\n",
    "\n",
    "# Round I - only put the required - skip if exists already\n",
    "for a_type in my_types:\n",
    "    print(a_type)\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    # find required field\n",
    "    schema_info = ff_utils.get_metadata('/profiles/{}.json'.format(a_type), key=transfer_key)\n",
    "    req_fields = schema_info['required']\n",
    "    ids = schema_info['identifyingProperties']\n",
    "    first_fields = list(set(req_fields+ids))\n",
    "    remove_existing_items = []\n",
    "    counter=0\n",
    "    print(len(store[a_type]), 'items exist on source')\n",
    "    for an_item in store[a_type]:\n",
    "        counter += 1\n",
    "        \n",
    "        if overwrite_existing:\n",
    "            post_first = {key:value for (key,value) in an_item.items() if key in first_fields}\n",
    "            ff_utils.post_metadata(post_first, obj_type, key = transfer_key)\n",
    "        else:\n",
    "            # does the item exist\n",
    "            exists = False\n",
    "            try:\n",
    "                # TODO check with all identifiers\n",
    "                existing = ff_utils.get_metadata(an_item['uuid'], key=transfer_key)\n",
    "                exists = True\n",
    "            except:\n",
    "                exists = False\n",
    "            # skip the items that exists\n",
    "            if exists and existing:\n",
    "                remove_existing_items.append(an_item['uuid'])\n",
    "                print(\"{} {} can not post item\".format(obj_type, an_item['uuid']))\n",
    "                continue\n",
    "            post_first = {key:value for (key,value) in an_item.items() if key in first_fields}\n",
    "            ff_utils.post_metadata(post_first, obj_type, key = transfer_key)\n",
    "   \n",
    "    second_round_items[a_type] = [i for i in store[a_type] if i['uuid'] not in remove_existing_items]\n",
    "    print(len(second_round_items[a_type]), 'items transfered to target')\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round II - patch the rest of the metadata\n",
    "for a_type in my_types:\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    if not second_round_items[a_type]:\n",
    "        continue \n",
    "    for an_item in second_round_items[a_type]:\n",
    "        counter += 1\n",
    "        ff_utils.patch_metadata(an_item, obj_id = an_item['uuid'], key = transfer_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round III - move attachments\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "#source_addresses\n",
    "source_health = ff_utils.get_metadata('/health', key = my_key)\n",
    "source_raw = source_health['file_upload_bucket'] \n",
    "source_pf = source_health['processed_file_bucket'] \n",
    "source_att = source_health['blob_bucket']\n",
    "\n",
    "#target_addresses\n",
    "target_health = ff_utils.get_metadata('/health', key = transfer_key)\n",
    "target_raw = target_health['file_upload_bucket'] \n",
    "target_pf = target_health['processed_file_bucket'] \n",
    "target_att = target_health['blob_bucket'] \n",
    "\n",
    "# Round III - move attachments\n",
    "for a_type in my_types:\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    for an_item in second_round_items[a_type]:\n",
    "        if 'attachment' in an_item.keys():\n",
    "            at_key = an_item['attachment']['blob_id']\n",
    "            copy_source = {'Bucket': source_att, 'Key': at_key}\n",
    "            try:\n",
    "                s3.meta.client.copy(copy_source, target_att, at_key)\n",
    "            except:\n",
    "                print('Can not find attachment on source', an_item['uuid'])\n",
    "                continue\n",
    "            print('attachment copied')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "#source_addresses\n",
    "source_health = ff_utils.get_metadata('/health', key = my_key)\n",
    "source_raw = source_health['file_upload_bucket'] \n",
    "source_pf = source_health['processed_file_bucket'] \n",
    "source_att = source_health['blob_bucket']\n",
    "\n",
    "#target_addresses\n",
    "target_health = ff_utils.get_metadata('/health', key = transfer_key)\n",
    "target_raw = target_health['file_upload_bucket'] \n",
    "target_pf = target_health['processed_file_bucket'] \n",
    "target_att = target_health['blob_bucket'] \n",
    "\n",
    "# Round IV - move files\n",
    "for a_type in my_types:\n",
    "    if a_type in ['file_processed']:\n",
    "        source_file_bucket = source_pf\n",
    "        target_file_bucket = target_pf\n",
    "    elif a_type in ['file_reference', 'file_fastq', 'file_microscopy', 'file_fasta', 'file_calibration']:\n",
    "        source_file_bucket = source_raw\n",
    "        target_file_bucket = target_raw\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    for an_item in second_round_items[a_type]:\n",
    "        # accumulate all keys from a file object to be uploaded\n",
    "        files_to_upload = []\n",
    "        file_resp = ff_utils.get_metadata(an_item['uuid'], key = my_key)\n",
    "        # add extra file keys\n",
    "        if file_resp.get('extra_files', []):\n",
    "            for an_extra_file in file_resp['extra_files']:\n",
    "                files_to_upload.append(an_extra_file['upload_key'])\n",
    "        # add main file key\n",
    "        files_to_upload.append(file_resp['upload_key'])\n",
    "        \n",
    "        for file_key in files_to_upload:\n",
    "            copy_source = {'Bucket': source_file_bucket, 'Key': file_key}\n",
    "            try:\n",
    "                s3.meta.client.copy(copy_source, target_file_bucket, file_key)\n",
    "            except:\n",
    "                print('Can not find file on source', file_key)\n",
    "                continue\n",
    "            print('file copied')\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
