{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING ON https://data.4dnucleome.org \n",
      "\n",
      "\n",
      "experiment_atacseq 2\n",
      "ontology_term 47\n",
      "individual_human 1\n",
      "award 4\n",
      "experiment_set_replicate 1\n",
      "biosource 1\n",
      "file_fastq 4\n",
      "quality_metric_fastqc 4\n",
      "user 12\n",
      "enzyme 1\n",
      "ontology 4\n",
      "biosample_cell_culture 2\n",
      "file_format 1\n",
      "lab 6\n",
      "biosample 2\n",
      "organism 1\n",
      "protocol 5\n",
      "vendor 2\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "### PLEASE COPY NOTEBOOKS TO YOUR FOLDERS TO PREVENT COMMIT CONFLICTS\n",
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "import json\n",
    "\n",
    "# get key from keypairs.json\n",
    "my_env = 'data'\n",
    "my_key = get_key('koray_data')\n",
    "schema_name = get_schema_names(my_key) \n",
    "print('WORKING ON', my_key['server'], '\\n')\n",
    "\n",
    "##### COLLECT ITEMS TO Release #####\n",
    "# use either a starting item to fetch all linked items\n",
    "\n",
    "# Use a starting item to find linked ones\n",
    "# starting_items = ['46db06ad-b399-4cf4-9acc-07b3e25ef132']\n",
    "#add_items = get_query_or_linked(my_key, linked=starting_items)\n",
    "\n",
    "# or a search query\n",
    "#my_query = '/search/?q=GOLD&type=Item&limit=all'\n",
    "#add_items = get_query_or_linked(my_key, query=my_query)\n",
    "\n",
    "# if you want you can dump them to separate json files (will work as test insert)\n",
    "# dump_to_json(add_items, destination folder)\n",
    "\n",
    "# my_query = '/search/?biosample.biosource.individual.organism.name=mouse&biosample.biosource_summary=ES-E14&experiment_type=in%20situ%20Hi-C&type=ExperimentHiC'\n",
    "# store = get_query_or_linked(my_key, query=my_query)\n",
    "# print(store.keys())\n",
    "# print(len([i['uuid'] for key in store for i in store[key]]))\n",
    "# print()\n",
    "\n",
    "find_linked = ['bc1bbd42-7486-4da7-bef1-006649040485']\n",
    "# store = get_query_or_linked(my_key, linked=find_linked, linked_frame='raw', ignore_field=['references', 'experiment_relation', 'biosample_relation'])\n",
    "\n",
    "store, uuid = record_object_es(find_linked, my_key, schema_name, store_frame='raw', add_pc_wfr=False, ignore_field=['experiment_relation', 'biosample_relation', 'references'])\n",
    "# print(store.keys())\n",
    "# for key in store:\n",
    "#     print(key, len(store[key]))\n",
    "# print(len([i['uuid'] for key in store for i in store[key]]))\n",
    "\n",
    "print()\n",
    "for key in store:\n",
    "    print(key, len(store[key]))\n",
    "print(len([i['uuid'] for key in store for i in store[key]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "12 items exist on source\n",
      "user d76eaf5b-549f-40b3-8731-9175793a85dd can not post item\n",
      "user 986b362f-4eb6-4a9c-8173-3ab267227777 can not post item\n",
      "user ab0b589c-01e1-4484-b58a-2a3bd9dbfe1e can not post item\n",
      "user 986b362f-4eb6-4a9c-8173-3ab267307e3a can not post item\n",
      "user 56682066-d5c0-48b5-aec9-9234c1135b22 can not post item\n",
      "user fb287a31-e765-41c5-8c1d-665f8e9f025b can not post item\n",
      "user e2324f87-0625-4bbc-803b-d47677aebe08 can not post item\n",
      "user 986b362f-4eb6-4a9c-8173-3ab267228139 can not post item\n",
      "user 986b362f-4eb6-4a9c-8173-3ab267111888 can not post item\n",
      "user 013f81b9-bd90-4d8b-b385-b75f6a10c7c0 can not post item\n",
      "user 68b09572-0c3e-48c7-90a9-7bad21d6bac1 can not post item\n",
      "user d8ac229e-ec08-4411-be38-dc779520ea62 can not post item\n",
      "0 items transfered to target\n",
      "\n",
      "award\n",
      "4 items exist on source\n",
      "award fcc7f634-9252-499f-b79c-380795af2ddd can not post item\n",
      "award b0b9c607-f8b4-4f02-93f4-9895b461334b can not post item\n",
      "award 12a92962-8265-4fc0-b2f8-cf14f05db58b can not post item\n",
      "award 029a2578-43dc-4343-8f41-694518cce304 can not post item\n",
      "0 items transfered to target\n",
      "\n",
      "lab\n",
      "6 items exist on source\n",
      "lab a7e19574-4def-456e-9de9-29dd7bdc2e68 can not post item\n",
      "lab 828cd4fe-ebb0-4b36-a94a-d2e3a36cc989 can not post item\n",
      "lab f9d7e470-cde0-4847-b629-2b70ae4e31cf can not post item\n",
      "lab a0812d8f-a4cc-4dbe-bd22-76e6e573b5ed can not post item\n",
      "lab ce54a7f7-af8a-4505-8327-e430634f494b can not post item\n",
      "lab abd48785-b0e5-4453-be14-30d37a516bf3 can not post item\n",
      "0 items transfered to target\n",
      "\n",
      "ontology\n",
      "4 items exist on source\n",
      "ontology 540026bc-8535-4448-903e-854af460b254 can not post item\n",
      "ontology 530016bc-8535-4448-903e-854af460b254 can not post item\n",
      "ontology 530026bc-8535-4448-903e-854af460b254 can not post item\n",
      "ontology 530006bc-8535-4448-903e-854af460b254 can not post item\n",
      "0 items transfered to target\n",
      "\n",
      "ontology_term\n",
      "47 items exist on source\n",
      "ontology_term 72e16a19-eef3-46ca-a1b8-20e646e69675 can not post item\n",
      "ontology_term 45d2b02e-130b-40db-8bf2-2288c6c57dcf can not post item\n",
      "ontology_term 111189bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term 111120bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term e5ca121b-78c2-4aa0-9d50-b698e412064f can not post item\n",
      "ontology_term e47e2bea-bc1e-4989-881c-521fc0b33529 can not post item\n",
      "ontology_term aea98752-596b-41c8-a917-464cbe3a46fb can not post item\n",
      "ontology_term 8edc79d6-bd1e-4623-aa86-3c7b6bbda324 can not post item\n",
      "ontology_term 870920a0-a6b7-496d-9db2-652f28a348c2 can not post item\n",
      "ontology_term 589d9712-953d-4d14-8de1-c56b3fa55478 can not post item\n",
      "ontology_term 3b8468df-e049-43e4-93e2-f62e565a7d74 can not post item\n",
      "ontology_term 3a3aca22-7824-4179-ba02-9fdc68abcc2f can not post item\n",
      "ontology_term 111118bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term 111116bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term 111114bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term 111113bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term 111112bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term f2cd7d4b-b484-4090-921c-e72e0d330aaa can not post item\n",
      "ontology_term cbb5e12f-d4dc-4019-905e-94ac9e2fe1db can not post item\n",
      "ontology_term c61b40c2-ffc9-493a-8b10-462718023240 can not post item\n",
      "ontology_term a27ad2c3-cd3b-46fe-b69b-9673eacdd9b0 can not post item\n",
      "ontology_term 8c95e354-29f2-4358-9d6c-346a749d2948 can not post item\n",
      "ontology_term 854ee606-f136-4755-94d9-88ce4d2309ce can not post item\n",
      "ontology_term 6fc493ac-5eae-4eed-9805-130be8163759 can not post item\n",
      "ontology_term 1d4c3da7-3250-449b-9a18-4b32cca5bc37 can not post item\n",
      "ontology_term 111117bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term 111115bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term 111111bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term 04216fbf-52a8-47c3-ac61-478f55c63ce9 can not post item\n",
      "ontology_term 01b974a0-5ff4-4d28-bca4-f8eadc406596 can not post item\n",
      "ontology_term e70754de-c0b7-46f7-9ee7-646d603735a3 can not post item\n",
      "ontology_term d724adac-7fe5-4c51-8da1-6ad58009c0fa can not post item\n",
      "ontology_term ac3c9a7b-cf94-43d3-9db0-898963651705 can not post item\n",
      "ontology_term 57f92515-ba4f-4275-86e7-35503d7e15dc can not post item\n",
      "ontology_term 4ac138bb-603d-46dc-860d-031486b651e1 can not post item\n",
      "ontology_term 45603d64-ac6b-4955-bef0-a35c831b67e3 can not post item\n",
      "ontology_term c088767b-edd9-4502-ace3-57ffc758a2e9 can not post item\n",
      "ontology_term a77b1635-45b0-4bed-b609-ed6ee39b5d47 can not post item\n",
      "ontology_term 3e3092da-d176-45e3-a15b-e3d685e3deda can not post item\n",
      "ontology_term 1a4374b9-9d9c-45fa-bd7e-5aca46379b80 can not post item\n",
      "7 items transfered to target\n",
      "\n",
      "organism\n",
      "1 items exist on source\n",
      "organism 7745b647-ff15-4ff3-9ced-b897d4e2983c can not post item\n",
      "0 items transfered to target\n",
      "\n",
      "file_format\n",
      "1 items exist on source\n",
      "file_format c13d06cf-218e-4f61-aaf0-91f226248b2c can not post item\n",
      "0 items transfered to target\n",
      "\n",
      "vendor\n",
      "2 items exist on source\n",
      "2 items transfered to target\n",
      "\n",
      "protocol\n",
      "5 items exist on source\n",
      "5 items transfered to target\n",
      "\n",
      "biosample_cell_culture\n",
      "2 items exist on source\n",
      "2 items transfered to target\n",
      "\n",
      "individual_human\n",
      "1 items exist on source\n",
      "1 items transfered to target\n",
      "\n",
      "biosource\n",
      "1 items exist on source\n",
      "1 items transfered to target\n",
      "\n",
      "enzyme\n",
      "1 items exist on source\n",
      "1 items transfered to target\n",
      "\n",
      "biosample\n",
      "2 items exist on source\n",
      "2 items transfered to target\n",
      "\n",
      "quality_metric_fastqc\n",
      "4 items exist on source\n",
      "4 items transfered to target\n",
      "\n",
      "file_fastq\n",
      "4 items exist on source\n",
      "4 items transfered to target\n",
      "\n",
      "experiment_atacseq\n",
      "2 items exist on source\n",
      "2 items transfered to target\n",
      "\n",
      "experiment_set_replicate\n",
      "1 items exist on source\n",
      "1 items transfered to target\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### This part should only run once!\n",
    "\n",
    "transfer_env = 'fourfront-webdev'\n",
    "transfer_key = ff_utils.get_authentication_with_server({}, ff_env=transfer_env)\n",
    "\n",
    "#### This part should only run once!\n",
    "\n",
    "# if the item exist in the target, should it overwrite it (will include user/award etc)\n",
    "overwrite_existing = False\n",
    "\n",
    "# reverse lookup dictionary for schema names\n",
    "rev_schema_name = {}\n",
    "for key, name in schema_name.items():\n",
    "    rev_schema_name[name] = schema_name[key]\n",
    "\n",
    "my_types = [i for i in ORDER if i in store.keys()]\n",
    "\n",
    "second_round_items = {}\n",
    "\n",
    "# Round I - only put the required - skip if exists already\n",
    "for a_type in my_types:\n",
    "    print(a_type)\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    # find required field\n",
    "    schema_info = ff_utils.get_metadata('/profiles/{}.json'.format(a_type), key=transfer_key)\n",
    "    req_fields = schema_info['required']\n",
    "    ids = schema_info['identifyingProperties']\n",
    "    first_fields = list(set(req_fields+ids))\n",
    "    remove_existing_items = []\n",
    "    counter=0\n",
    "    print(len(store[a_type]), 'items exist on source')\n",
    "    for an_item in store[a_type]:\n",
    "        counter += 1\n",
    "        \n",
    "        if overwrite_existing:\n",
    "            post_first = {key:value for (key,value) in an_item.items() if key in first_fields}\n",
    "            ff_utils.post_metadata(post_first, obj_type, key = transfer_key)\n",
    "        else:\n",
    "            # does the item exist\n",
    "            exists = False\n",
    "            try:\n",
    "                # TODO check with all identifiers\n",
    "                existing = ff_utils.get_metadata(an_item['uuid'], key=transfer_key)\n",
    "                exists = True\n",
    "            except:\n",
    "                exists = False\n",
    "            # skip the items that exists\n",
    "            if exists and existing:\n",
    "                remove_existing_items.append(an_item['uuid'])\n",
    "                print(\"{} {} can not post item\".format(obj_type, an_item['uuid']))\n",
    "                continue\n",
    "            post_first = {key:value for (key,value) in an_item.items() if key in first_fields}\n",
    "            ff_utils.post_metadata(post_first, obj_type, key = transfer_key)\n",
    "   \n",
    "    second_round_items[a_type] = [i for i in store[a_type] if i['uuid'] not in remove_existing_items]\n",
    "    print(len(second_round_items[a_type]), 'items transfered to target')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round II - patch the rest of the metadata\n",
    "for a_type in my_types:\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    if not second_round_items[a_type]:\n",
    "        continue \n",
    "    for an_item in second_round_items[a_type]:\n",
    "        counter += 1\n",
    "        if a_type == 'file_fastq':\n",
    "            if 'extra_files' in an_item:\n",
    "                del an_item['extra_files']\n",
    "        ff_utils.patch_metadata(an_item, obj_id = an_item['uuid'], key = transfer_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attachment copied\n",
      "attachment copied\n",
      "attachment copied\n"
     ]
    }
   ],
   "source": [
    "# Round III - move attachments\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "#source_addresses\n",
    "source_health = ff_utils.get_metadata('/health', key = my_key)\n",
    "source_raw = source_health['file_upload_bucket'] \n",
    "source_pf = source_health['processed_file_bucket'] \n",
    "source_att = source_health['blob_bucket']\n",
    "\n",
    "#target_addresses\n",
    "target_health = ff_utils.get_metadata('/health', key = transfer_key)\n",
    "target_raw = target_health['file_upload_bucket'] \n",
    "target_pf = target_health['processed_file_bucket'] \n",
    "target_att = target_health['blob_bucket'] \n",
    "\n",
    "# Round III - move attachments\n",
    "for a_type in my_types:\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    for an_item in second_round_items[a_type]:\n",
    "        if 'attachment' in an_item.keys():\n",
    "            at_key = an_item['attachment']['blob_id']\n",
    "            copy_source = {'Bucket': source_att, 'Key': at_key}\n",
    "            try:\n",
    "                s3.meta.client.copy(copy_source, target_att, at_key)\n",
    "            except:\n",
    "                print('Can not find attachment on source', an_item['uuid'])\n",
    "                continue\n",
    "            print('attachment copied')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n"
     ]
    }
   ],
   "source": [
    "# Round IV - move files\n",
    "for a_type in my_types:\n",
    "    if a_type in ['file_processed']:\n",
    "        source_file_bucket = source_pf\n",
    "        target_file_bucket = target_pf\n",
    "    elif a_type in ['file_reference', 'file_fastq', 'file_microscopy', 'file_fasta', 'file_calibration']:\n",
    "        source_file_bucket = source_raw\n",
    "        target_file_bucket = target_raw\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    for an_item in second_round_items[a_type]:\n",
    "        # accumulate all keys from a file object to be uploaded\n",
    "        files_to_upload = []\n",
    "        file_resp = ff_utils.get_metadata(an_item['uuid'], key = my_key)\n",
    "        # add extra file keys\n",
    "        if file_resp.get('extra_files', []):\n",
    "            for an_extra_file in file_resp['extra_files']:\n",
    "                files_to_upload.append(an_extra_file['upload_key'])\n",
    "        # add main file key\n",
    "        files_to_upload.append(file_resp['upload_key'])\n",
    "        \n",
    "        for file_key in files_to_upload:\n",
    "            copy_source = {'Bucket': source_file_bucket, 'Key': file_key}\n",
    "            try:\n",
    "                s3.meta.client.copy(copy_source, target_file_bucket, file_key)\n",
    "            except:\n",
    "                print('Can not find file on source', file_key)\n",
    "                continue\n",
    "            print('file copied')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
